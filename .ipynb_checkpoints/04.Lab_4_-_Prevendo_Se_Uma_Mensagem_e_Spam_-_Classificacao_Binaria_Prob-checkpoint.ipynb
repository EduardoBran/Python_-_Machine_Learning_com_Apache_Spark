{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45aad633",
   "metadata": {},
   "source": [
    "<span style=\"color: green; font-size: 40px; font-weight: bold;\">Lab 4 (Classificação Binária Probabilística) </span>\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "# Prevendo Se Uma Mensagem de Texto é Spam\n",
    "\n",
    "<br>\n",
    "\n",
    "### Contexto\n",
    "\n",
    "Neste projeto, abordaremos um problema comum em sistemas de comunicação digital: a **detecção de mensagens de texto indesejadas, conhecidas como spam**. Com o aumento do uso de mensagens eletrônicas, a capacidade de distinguir automaticamente entre mensagens legítimas e spam se tornou essencial para manter a integridade e a usabilidade dos serviços de mensagens. O objetivo deste projeto é desenvolver um modelo preditivo que, baseado no conteúdo textual das mensagens, consiga identificar se uma mensagem é spam ou não. Utilizaremos técnicas de Machine Learning para criar um modelo de classificação binária probabilística, capaz de fornecer uma previsão acompanhada de uma estimativa de probabilidade.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Objetivo\n",
    "\n",
    "O objetivo deste projeto é **construir um modelo de Machine Learning capaz de prever se uma mensagem de texto é spam**. O modelo será treinado utilizando dados históricos de mensagens rotuladas como \"spam\" ou \"ham\" (não spam), permitindo que ele faça previsões sobre novas mensagens com base em padrões aprendidos.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Pergunta de Negócio Principal\n",
    "\n",
    "> \"Como podemos prever se uma mensagem de texto é spam utilizando seu conteúdo textual?\"\n",
    "\n",
    "<br>\n",
    "\n",
    "### Entregável\n",
    "\n",
    "O entregável deste projeto será um **modelo de Machine Learning treinado para identificar mensagens de texto como spam ou não**. O modelo será capaz de classificar novas mensagens e fornecer a probabilidade associada a cada previsão. O processo de desenvolvimento incluirá a preparação dos dados, a seleção de features relevantes, o treinamento do modelo e a avaliação de seu desempenho.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Sobre o Conjunto de Dados\n",
    "\n",
    "Os dados utilizados neste projeto contêm uma coleção de mensagens de texto classificadas manualmente como \"spam\" ou \"ham\". Cada entrada do conjunto de dados inclui o texto da mensagem e sua respectiva classificação. Utilizaremos esse conjunto para treinar e validar nosso modelo.\n",
    "\n",
    "<br>\n",
    "<table border=\"2\">\n",
    "  <tr>\n",
    "    <th style=\"text-align: center; font-size: 16px;\">Nome da Coluna</th>\n",
    "    <th style=\"text-align: center; font-size: 16px;\">Tipo de Dado</th>\n",
    "    <th style=\"text-align: center; font-size: 16px;\">Descrição</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>label</td>\n",
    "    <td>string</td>\n",
    "      <td>Classificação da mensagem (<b>ham</b> para <i>não spam</i> e <b>spam</b> para <i>mensagens indesejadas</i>).</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>message</td>\n",
    "    <td>string</td>\n",
    "    <td>Conteúdo textual da mensagem.</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "# Importando Pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a18a77bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa o findspark e inicializa\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import IDF, HashingTF, Tokenizer\n",
    "from pyspark.ml.classification import NaiveBayes, NaiveBayesModel\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c985b686",
   "metadata": {},
   "source": [
    "<br> <br>\n",
    "\n",
    "# <span style=\"color: green; font-size: 38px; font-weight: bold;\">Preparando o Ambiente Spark</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "203f07a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/12 18:04:46 WARN Utils: Your hostname, eduardo-Inspiron-15-3520 resolves to a loopback address: 127.0.1.1; using 192.168.0.13 instead (on interface wlp0s20f3)\n",
      "24/08/12 18:04:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/12 18:04:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/08/12 18:04:47 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/08/12 18:04:47 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.13:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Lab4</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fbbc1a86130>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definindo semente aleatória (seed) para reprodutibilidade do notebook\n",
    "rnd_seed = 23\n",
    "np.random.seed = rnd_seed\n",
    "np.random.set_state = rnd_seed\n",
    "\n",
    "# Se houver uma sessão Spark ativa, encerre-a\n",
    "if 'sc' in globals():\n",
    "    sc.stop()\n",
    "\n",
    "if 'spark' in globals():\n",
    "    spark.stop()\n",
    "\n",
    "\n",
    "# Criando o Spark Context\n",
    "conf = SparkConf().setAppName(\"Lab4\") \\\n",
    "                  .set(\"spark.ui.showConsoleProgress\", \"false\") \\\n",
    "                  .set(\"spark.executor.heartbeatInterval\", \"20s\") \\\n",
    "                  .set(\"spark.eventLog.enabled\", \"false\") \\\n",
    "                  .set(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
    "                  .set(\"spark.sql.debug.maxToStringFields\", \"100\") \\\n",
    "                  .set(\"spark.executor.memory\", \"4g\") \\\n",
    "                  .set(\"spark.driver.memory\", \"4g\") \\\n",
    "                  .set(\"spark.driver.maxResultSize\", \"2g\")  # Configuração adicional para limitar o tamanho do resultado\n",
    "\n",
    "# Criar o Spark Context e a Spark Session\n",
    "sc = SparkContext(conf=conf)\n",
    "spSession = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# Ajustar o nível de log para ERROR\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Configurar log4j para suprimir avisos (deixar como comentário e volta ao normal)\n",
    "log4j_logger = sc._jvm.org.apache.log4j\n",
    "log4j_logger.LogManager.getLogger(\"org\").setLevel(log4j_logger.Level.ERROR)\n",
    "log4j_logger.LogManager.getLogger(\"akka\").setLevel(log4j_logger.Level.ERROR)\n",
    "\n",
    "# Visualizar o objeto spark_session\n",
    "spSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb531be",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <span style=\"color: green; font-size: 38px; font-weight: bold;\">Carregando os Dados</span>\n",
    "\n",
    "- Os dados serão carregados a partir de um arquivo CSV e gerados como um RDD (Resilient Distributed Dataset) no Apache Spark. O RDD é uma estrutura de dados distribuída que permite o processamento paralelo em um cluster, otimizando a performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87ab1c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'> \n",
      "\n",
      "Lab/dados/dataset4.csv MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 \n",
      "\n",
      "1000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Carregando os dados e gerando um RDD\n",
    "spamRDD = sc.textFile(\"Lab/dados/dataset4.csv\")\n",
    "\n",
    "# Tipo\n",
    "print(type(spamRDD), '\\n')\n",
    "\n",
    "# Colocando o RDD em cache. Esse processo otimiza a performance\n",
    "print(spamRDD.cache(), '\\n')\n",
    "\n",
    "# Número de registros\n",
    "print(spamRDD.count(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f46d0eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham,Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...,,,,,,,,,', 'ham,Ok lar... Joking wif u oni...,,,,,,,,,,', 'ham,U dun say so early hor... U c already then say...,,,,,,,,,,', \"ham,Nah I don't think he goes to usf, he lives around here though,,,,,,,,,\", 'ham,Even my brother is not like to speak with me. They treat me like aids patent.,,,,,,,,,,']\n"
     ]
    }
   ],
   "source": [
    "# Visualizando as primeiras linhas\n",
    "print(spamRDD.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59b679fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module was compiled against NumPy C-API version 0x10 (NumPy 1.23) but the running NumPy has C-API version 0xf. Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module was compiled against NumPy C-API version 0x10 (NumPy 1.23) but the running NumPy has C-API version 0xf. Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem."
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...,,,,,,,,,,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham            Ok lar... Joking wif u oni...,,,,,,,,,,\n",
       "2   ham  U dun say so early hor... U c already then say...\n",
       "3   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "4   ham  Even my brother is not like to speak with me. ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Visualizando primeiras 4 linhas com Pandas (Apenas para visualização)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Obter as primeiras 5 linhas do RDD\n",
    "linhas = spamRDD.take(5)\n",
    "\n",
    "# Dividir as linhas em colunas utilizando a primeira vírgula como delimitador\n",
    "dados_formatados = [linha.split(\",\", 1) for linha in linhas]\n",
    "\n",
    "# Criar um DataFrame Pandas com as colunas e os dados formatados\n",
    "df = pd.DataFrame(dados_formatados, columns=[\"label\", \"message\"])\n",
    "\n",
    "# Mostrar o DataFrame\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7328d1",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "\n",
    "# <span style=\"color: green; font-size: 38px; font-weight: bold;\"> Análise Exploratória Inicial dos Dados </span>\n",
    "\n",
    "<br>\n",
    "\n",
    "### Criação de Função Para Análise Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c08296a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A função foi criada com sucesso.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def funcao_analise_inicial(df):\n",
    "    # Configurar Pandas para exibir todas as linhas\n",
    "    pd.set_option('display.max_rows', None)\n",
    "\n",
    "    # Informações do DataFrame\n",
    "    print('\\n\\n INFO \\n\\n')\n",
    "    df.info()\n",
    "    print('\\n\\n ------------------------------------------------------------------------------------------ \\n\\n')\n",
    "\n",
    "    # Verifica se há valores ausentes e duplicados\n",
    "    valores_ausentes = df.isna().sum().sum() > 0\n",
    "    valores_duplicados = df.duplicated().sum() > 0\n",
    "\n",
    "    # Nomes das variáveis com valores ausentes\n",
    "    variaveis_ausentes = df.columns[df.isna().any()].tolist()\n",
    "\n",
    "    # Número de linhas duplicadas\n",
    "    num_linhas_duplicadas = df.duplicated().sum()\n",
    "\n",
    "    # Porcentagem de linhas duplicadas\n",
    "    porcentagem_linhas_duplicadas = (num_linhas_duplicadas / len(df)) * 100\n",
    "\n",
    "    # Exibe o resultado sobre valores ausentes e duplicados\n",
    "    print(\"\\n\\nExistem valores ausentes:\", valores_ausentes)\n",
    "    if valores_ausentes:\n",
    "        print(\"\\nVariáveis com valores ausentes:\", variaveis_ausentes)\n",
    "    else:\n",
    "        print(\"\\nNenhuma variável possui valores ausentes.\")\n",
    "\n",
    "    print(\"\\n\\nExistem valores duplicados:\", valores_duplicados)\n",
    "    if valores_duplicados:\n",
    "        print(\"\\nNúmero de Linhas Duplicadas:\", num_linhas_duplicadas)\n",
    "        print(\"\\nPorcentagem de Linhas Duplicadas: {:.2f}%\".format(porcentagem_linhas_duplicadas))\n",
    "    else:\n",
    "        print(\"\\nNenhuma variável possui valores duplicados.\")\n",
    "    \n",
    "    # Verificação de caracteres especiais\n",
    "    caracteres_especiais = re.compile('[@_!#$%^&*<>()?/\\\\|}{~:]')   # nenhum caracter removido\n",
    "    colunas_com_caracteres_especiais = {}\n",
    "\n",
    "    for coluna in df.columns:\n",
    "        if df[coluna].dtype == 'object':  # Verifica apenas colunas de texto\n",
    "            contem_caracteres_especiais = df[coluna].apply(lambda x: bool(caracteres_especiais.search(x) if isinstance(x, str) else False)).any()\n",
    "            if contem_caracteres_especiais:\n",
    "                indices_com_caracteres_especiais = df[coluna][df[coluna].apply(lambda x: bool(caracteres_especiais.search(x) if isinstance(x, str) else False))].index.tolist()\n",
    "                colunas_com_caracteres_especiais[coluna] = indices_com_caracteres_especiais\n",
    "\n",
    "    # Exibe o resultado sobre caracteres especiais\n",
    "    print(\"\\n\\nExistem caracteres especiais nas colunas:\", bool(colunas_com_caracteres_especiais))\n",
    "    if colunas_com_caracteres_especiais:\n",
    "        print(\"\\nColunas com caracteres especiais e os índices:\")\n",
    "        for coluna, indices in colunas_com_caracteres_especiais.items():\n",
    "            print(f\"\\n Coluna [ {coluna} ]: Índices com caracteres especiais {indices}\")\n",
    "    else:\n",
    "        print(\"\\nNenhuma coluna possui caracteres especiais.\")\n",
    "\n",
    "print('A função foi criada com sucesso.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792be9a3",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Transformando dados carregados em RDD para dataframe do Pandas (apenas para Análise Inicial)\n",
    "\n",
    "- Vamos realizar análise exploratória através da função acima. RDDs são ótimos para processamento, mas ruins para exploração, então converteremos o RDD para DataFrame Spark e então para DataFrame Pandas (**não é possível converter diretamente objeto RDD para objeto Pandas**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b16e9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'> \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...,,,,,,,,,,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham            Ok lar... Joking wif u oni...,,,,,,,,,,\n",
       "2   ham  U dun say so early hor... U c already then say...\n",
       "3   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "4   ham  Even my brother is not like to speak with me. ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Definindo as colunas manualmente\n",
    "colunas = [\"label\", \"message\"]\n",
    "\n",
    "# Dividindo as linhas corretamente usando a vírgula como delimitador e criando Rows\n",
    "dados_formatados = spamRDD.map(lambda linha: linha.split(\",\", 1)).map(lambda x: Row(label=x[0], message=x[1]))\n",
    "\n",
    "# Criando o DataFrame do PySpark com as colunas definidas manualmente\n",
    "df_spark = spSession.createDataFrame(dados_formatados, colunas)\n",
    "\n",
    "# Verificar o tipo do objeto\n",
    "print(type(df_spark), '\\n')\n",
    "\n",
    "# Converte DataFrame Spark para DataFrame Pandas\n",
    "df_pandas = df_spark.toPandas()\n",
    "\n",
    "# Visualizando as primeiras linhas do DataFrame Pandas\n",
    "display(df_pandas.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c67f4a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Visualizando Função para Análise Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f247874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " INFO \n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   label    1000 non-null   object\n",
      " 1   message  1000 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 15.8+ KB\n",
      "\n",
      "\n",
      " ------------------------------------------------------------------------------------------ \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Existem valores ausentes: False\n",
      "\n",
      "Nenhuma variável possui valores ausentes.\n",
      "\n",
      "\n",
      "Existem valores duplicados: True\n",
      "\n",
      "Número de Linhas Duplicadas: 53\n",
      "\n",
      "Porcentagem de Linhas Duplicadas: 5.30%\n",
      "\n",
      "\n",
      "Existem caracteres especiais nas colunas: True\n",
      "\n",
      "Colunas com caracteres especiais e os índices:\n",
      "\n",
      " Coluna [ message ]: Índices com caracteres especiais [5, 6, 8, 9, 12, 15, 16, 19, 20, 21, 23, 25, 30, 32, 33, 34, 37, 41, 44, 48, 51, 53, 57, 58, 62, 63, 66, 71, 75, 76, 77, 78, 79, 81, 84, 86, 87, 92, 94, 97, 98, 99, 101, 103, 107, 108, 112, 118, 119, 121, 123, 128, 130, 132, 134, 137, 139, 142, 146, 147, 149, 153, 155, 159, 160, 162, 165, 166, 167, 168, 170, 174, 176, 177, 180, 182, 185, 194, 196, 199, 200, 202, 203, 208, 214, 215, 220, 221, 224, 228, 236, 237, 238, 245, 246, 248, 249, 256, 257, 258, 259, 262, 264, 265, 267, 269, 271, 273, 275, 276, 277, 279, 283, 284, 286, 287, 290, 292, 293, 302, 303, 304, 307, 308, 312, 314, 317, 318, 321, 322, 327, 332, 333, 334, 337, 342, 346, 348, 349, 351, 359, 360, 361, 364, 366, 369, 370, 371, 374, 376, 377, 380, 381, 383, 384, 386, 388, 395, 399, 400, 401, 404, 405, 406, 409, 410, 411, 413, 415, 420, 423, 424, 425, 428, 431, 433, 435, 436, 437, 438, 441, 443, 449, 451, 457, 458, 462, 466, 469, 477, 479, 481, 484, 485, 486, 487, 488, 489, 492, 497, 498, 500, 501, 502, 503, 504, 505, 506, 507, 508, 510, 511, 513, 514, 515, 517, 518, 519, 520, 521, 522, 523, 524, 525, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 556, 557, 558, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 578, 579, 580, 581, 584, 585, 586, 587, 588, 589, 592, 593, 594, 596, 597, 599, 600, 601, 603, 605, 606, 608, 610, 611, 612, 613, 614, 616, 617, 618, 619, 620, 621, 623, 625, 628, 629, 630, 631, 632, 633, 634, 635, 637, 638, 639, 640, 642, 643, 644, 645, 646, 647, 648, 650, 651, 652, 653, 654, 655, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 671, 672, 674, 675, 676, 677, 678, 679, 681, 682, 684, 685, 686, 687, 689, 690, 691, 692, 694, 695, 696, 698, 699, 700, 701, 702, 703, 704, 705, 706, 708, 709, 711, 712, 713, 714, 715, 719, 720, 721, 722, 723, 724, 726, 727, 728, 730, 731, 732, 733, 734, 735, 736, 737, 739, 740, 741, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 756, 758, 759, 761, 762, 763, 764, 765, 766, 767, 768, 770, 771, 773, 774, 776, 777, 778, 779, 780, 781, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 796, 798, 799, 800, 801, 802, 804, 805, 806, 809, 810, 812, 814, 816, 818, 819, 820, 821, 822, 823, 824, 825, 826, 829, 830, 831, 832, 833, 834, 835, 836, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 850, 851, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 865, 866, 867, 868, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 882, 883, 884, 887, 888, 890, 894, 895, 896, 897, 898, 899, 902, 904, 905, 906, 907, 909, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 926, 928, 929, 930, 931, 932, 934, 935, 936, 937, 939, 940, 941, 942, 944, 945, 947, 948, 950, 952, 953, 954, 956, 957, 958, 959, 960, 961, 964, 965, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 984, 985, 986, 987, 988, 989, 990, 992, 993, 994, 995, 997, 998, 999]\n"
     ]
    }
   ],
   "source": [
    "funcao_analise_inicial(df_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8491c278",
   "metadata": {},
   "source": [
    "### Resumo\n",
    "\n",
    "- Necessário tratamento para conversão para tipo numérico das duas colunas.\n",
    "\n",
    "<br> <br> <br>\n",
    "\n",
    "# <span style=\"color: green; font-size: 38px; font-weight: bold;\">Transformação dos Dados</span>\n",
    "\n",
    "<br>\n",
    "\n",
    "Nesta etapa trataremos da conversão para tipo de numérico da primeira coluna.\n",
    "\n",
    "<br>\n",
    "\n",
    "## Tratando Primeira Coluna\n",
    "\n",
    "- Será criado uma função para a conversão da primeira coluna para o tipo numérico\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Criando e Aplicando a Função (diretamente no objeto RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b86703e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de transformação\n",
    "def TransformToVector(inputStr):\n",
    "    \n",
    "    # Separa as colunas\n",
    "    attList = inputStr.split(\",\")\n",
    "    \n",
    "    # Ajusta o label (target)\n",
    "    smsType = 0.0 if attList[0] == \"ham\" else 1.0\n",
    "    \n",
    "    return [smsType, attList[1]]\n",
    "\n",
    "# Aplica a função\n",
    "spamRDD2 = spamRDD.map(TransformToVector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83521c50",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "\n",
    "# <span style=\"color: green; font-size: 38px; font-weight: bold;\"> Análise Exploratória</span>\n",
    "\n",
    "<br>\n",
    "\n",
    "- Realizando uma nova etapa de <i>Análise Exploratória</i> agora com os dados já <i>tratados</i>.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Convertendo Para Dataframe do Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6779f1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- message: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Converte o RDD em DataFrame\n",
    "spamDF = spSession.createDataFrame(spamRDD2, [\"label\", \"message\"])\n",
    "print(type(spamDF))\n",
    "\n",
    "# Exibir a estrutura do DataFrame para confirmação\n",
    "print(spamDF.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fee13d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|             message|\n",
      "+-----+--------------------+\n",
      "|  0.0|Go until jurong p...|\n",
      "|  0.0|Ok lar... Joking ...|\n",
      "|  0.0|U dun say so earl...|\n",
      "|  0.0|Nah I don't think...|\n",
      "|  0.0|Even my brother i...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verificando primeiras linhas\n",
    "spamDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afb56ad",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "# Pré-Processamento de Dados Para Construção de Modelos de Machine Learning\n",
    "\n",
    "- O objeto já foi convertido para dataframe do pyspark na etapa anterior (spamDF).\n",
    "\n",
    "<br><br>\n",
    "\n",
    "## Processamento de Linguagem Natural\n",
    "\n",
    "Aqui nós criamos os objetos que permitirão transformar os dados de texto em uma matriz numérica.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aadd63ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.ml.feature.Tokenizer'> \n",
      "\n",
      "<class 'pyspark.ml.feature.HashingTF'> \n",
      "\n",
      "<class 'pyspark.ml.feature.IDF'> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cria o Tokenizador \n",
    "tokenizador = Tokenizer(inputCol = \"message\", outputCol = \"words\")\n",
    "print(type(tokenizador), '\\n')\n",
    "\n",
    "# Aplica o TF (Term Frequency) para extrair a frequência de cada termo nas linhas de texto\n",
    "term_frequency = HashingTF(inputCol = tokenizador.getOutputCol(), outputCol = \"tempfeatures\")\n",
    "print(type(term_frequency), '\\n')\n",
    "\n",
    "# Aplica o IDF (Inverse Document Frequency) para calcular a frequência inversa dos documentos\n",
    "inverse_tf = IDF(inputCol = term_frequency.getOutputCol(), outputCol = \"features\")\n",
    "print(type(inverse_tf), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaef744",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Dividindo os dados em Dados de Treino e Dados de Teste\n",
    "- Nós **treinamos** o modelo com **dados de treino** e **avaliamos** o modelo com **dados de teste**.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fccd987a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "692\n",
      "308\n"
     ]
    }
   ],
   "source": [
    "# Dados de Treino e de Teste\n",
    "(dados_treino, dados_teste) = spamDF.randomSplit([0.7, 0.3])\n",
    "\n",
    "print(dados_treino.count())\n",
    "print(dados_teste.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75cb202",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>\n",
    "\n",
    "<span style=\"color: green; font-size: 40px; font-weight: bold;\">Construindo Modelos de Machine Learning</span>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "## Criando Dataframe para salvar métricas de cada Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ef3db6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um dataframe para receber as métricas de cada modelo\n",
    "df_modelos = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f031aeb8",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "\n",
    "# <span style=\"color: green; font-weight: bold;\">Modelo 1 com Regressão Logística</span>\n",
    "\n",
    "<br>\n",
    "\n",
    "> # Versão 1\n",
    "\n",
    "- Sem Ajuste de Hiperparâmetros\n",
    "\n",
    "<br>\n",
    "\n",
    "### Criação do Pipeline e Criação, Treinamento, Previsão e Avaliação do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9cdf126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modelo</th>\n",
       "      <th>Versão</th>\n",
       "      <th>Tipo de Modelo</th>\n",
       "      <th>Acurácia</th>\n",
       "      <th>Precisão</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>1</td>\n",
       "      <td>Sem Ajuste de Hiperparâmetros</td>\n",
       "      <td>0.883117</td>\n",
       "      <td>0.895346</td>\n",
       "      <td>0.883117</td>\n",
       "      <td>0.881567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Modelo Versão                 Tipo de Modelo  Acurácia  \\\n",
       "0  LogisticRegression      1  Sem Ajuste de Hiperparâmetros  0.883117   \n",
       "\n",
       "   Precisão    Recall  F1-Score  \n",
       "0  0.895346  0.883117  0.881567  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Criação do modelo\n",
    "lrClassifier = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Criação do Pipeline\n",
    "pipeline_lr = Pipeline(stages=[tokenizador, term_frequency, inverse_tf, lrClassifier])\n",
    "\n",
    "# Treinamento do modelo com o Pipeline\n",
    "modelo_lr = pipeline_lr.fit(dados_treino)\n",
    "\n",
    "# Previsões nos dados de teste\n",
    "previsoes_lr = modelo_lr.transform(dados_teste)\n",
    "\n",
    "# Avaliando as métricas\n",
    "avaliador_acuracia_lr = MulticlassClassificationEvaluator(predictionCol=\"prediction\", \n",
    "                                                          labelCol=\"label\", \n",
    "                                                          metricName=\"accuracy\")\n",
    "\n",
    "avaliador_precisao_lr = MulticlassClassificationEvaluator(predictionCol=\"prediction\", \n",
    "                                                          labelCol=\"label\", \n",
    "                                                          metricName=\"weightedPrecision\")\n",
    "\n",
    "avaliador_recall_lr = MulticlassClassificationEvaluator(predictionCol=\"prediction\", \n",
    "                                                        labelCol=\"label\", \n",
    "                                                        metricName=\"weightedRecall\")\n",
    "\n",
    "avaliador_f1_lr = MulticlassClassificationEvaluator(predictionCol=\"prediction\", \n",
    "                                                    labelCol=\"label\", \n",
    "                                                    metricName=\"f1\")\n",
    "\n",
    "# Calculando as métricas\n",
    "acuracia_lr = avaliador_acuracia_lr.evaluate(previsoes_lr)\n",
    "precisao_lr = avaliador_precisao_lr.evaluate(previsoes_lr)\n",
    "recall_lr = avaliador_recall_lr.evaluate(previsoes_lr)\n",
    "f1_score_lr = avaliador_f1_lr.evaluate(previsoes_lr)\n",
    "\n",
    "# Salvando as métricas no DataFrame\n",
    "modelo_lr = pd.DataFrame({\n",
    "    'Modelo': ['LogisticRegression'],\n",
    "    'Versão': ['1'],\n",
    "    'Tipo de Modelo': ['Sem Ajuste de Hiperparâmetros'],\n",
    "    'Acurácia': [acuracia_lr],\n",
    "    'Precisão': [precisao_lr],\n",
    "    'Recall': [recall_lr],\n",
    "    'F1-Score': [f1_score_lr]\n",
    "})\n",
    "\n",
    "# Concatenando com o DataFrame existente\n",
    "df_modelos = pd.concat([df_modelos, modelo_lr], ignore_index=True)\n",
    "\n",
    "# Visualizando o DataFrame com as métricas\n",
    "display(df_modelos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9275f4",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "\n",
    "# <span style=\"color: green; font-weight: bold;\">Modelo 2 com NaiveBayes</span>\n",
    "\n",
    "<br>\n",
    "\n",
    "> # Versão 1\n",
    "\n",
    "- Sem Ajuste de Hiperparâmetros\n",
    "\n",
    "<br>\n",
    "\n",
    "### Criação do Pipeline e Criação, Treinamento, Previsão e Avaliação do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1b25d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modelo</th>\n",
       "      <th>Versão</th>\n",
       "      <th>Tipo de Modelo</th>\n",
       "      <th>Acurácia</th>\n",
       "      <th>Precisão</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>1</td>\n",
       "      <td>Sem Ajuste de Hiperparâmetros</td>\n",
       "      <td>0.883117</td>\n",
       "      <td>0.895346</td>\n",
       "      <td>0.883117</td>\n",
       "      <td>0.881567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaiveBayes</td>\n",
       "      <td>1</td>\n",
       "      <td>Sem Ajuste de Hiperparâmetros</td>\n",
       "      <td>0.905844</td>\n",
       "      <td>0.909153</td>\n",
       "      <td>0.905844</td>\n",
       "      <td>0.905883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Modelo Versão                 Tipo de Modelo  Acurácia  \\\n",
       "0  LogisticRegression      1  Sem Ajuste de Hiperparâmetros  0.883117   \n",
       "1          NaiveBayes      1  Sem Ajuste de Hiperparâmetros  0.905844   \n",
       "\n",
       "   Precisão    Recall  F1-Score  \n",
       "0  0.895346  0.883117  0.881567  \n",
       "1  0.909153  0.905844  0.905883  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Criação do modelo\n",
    "nbClassifier = NaiveBayes()\n",
    "\n",
    "# Criação do Pipeline\n",
    "pipeline = Pipeline(stages=[tokenizador, term_frequency, inverse_tf, nbClassifier])\n",
    "\n",
    "# Treinamento do modelo com o Pipeline\n",
    "modelo = pipeline.fit(dados_treino)\n",
    "\n",
    "# Previsões nos dados de teste\n",
    "previsoes = modelo.transform(dados_teste)\n",
    "\n",
    "# Avaliando as métricas\n",
    "avaliador_acuracia = MulticlassClassificationEvaluator(predictionCol=\"prediction\", \n",
    "                                                       labelCol=\"label\", \n",
    "                                                       metricName=\"accuracy\")\n",
    "\n",
    "avaliador_precisao = MulticlassClassificationEvaluator(predictionCol=\"prediction\", \n",
    "                                                       labelCol=\"label\", \n",
    "                                                       metricName=\"weightedPrecision\")\n",
    "\n",
    "avaliador_recall = MulticlassClassificationEvaluator(predictionCol=\"prediction\", \n",
    "                                                     labelCol=\"label\", \n",
    "                                                     metricName=\"weightedRecall\")\n",
    "\n",
    "avaliador_f1 = MulticlassClassificationEvaluator(predictionCol=\"prediction\", \n",
    "                                                 labelCol=\"label\", \n",
    "                                                 metricName=\"f1\")\n",
    "\n",
    "# Calculando as métricas\n",
    "acuracia = avaliador_acuracia.evaluate(previsoes)\n",
    "precisao = avaliador_precisao.evaluate(previsoes)\n",
    "recall = avaliador_recall.evaluate(previsoes)\n",
    "f1_score = avaliador_f1.evaluate(previsoes)\n",
    "\n",
    "# Salvando as métricas no DataFrame\n",
    "modelo_nb = pd.DataFrame({\n",
    "    'Modelo': ['NaiveBayes'],\n",
    "    'Versão': ['1'],\n",
    "    'Tipo de Modelo': ['Sem Ajuste de Hiperparâmetros'],\n",
    "    'Acurácia': [acuracia],\n",
    "    'Precisão': [precisao],\n",
    "    'Recall': [recall],\n",
    "    'F1-Score': [f1_score]\n",
    "})\n",
    "\n",
    "# Concatenando com o DataFrame existente\n",
    "df_modelos = pd.concat([df_modelos, modelo_nb], ignore_index=True)\n",
    "\n",
    "# Visualizando o DataFrame com as métricas\n",
    "display(df_modelos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0bed91",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "\n",
    "# Fim!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
